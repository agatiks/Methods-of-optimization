{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agatiks/Methods-of-optimization/blob/main/lab2_StochasticGradientDescent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0k3PLU0JO0U"
      },
      "source": [
        "# Задание 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzeI5oopIuJi"
      },
      "source": [
        "Возьмём датасет, содержащий 200 записей о росте (в дюймах) и весе (в фунтах) 18-летних людей с этого сайта http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_Dinov_020108_HeightsWeights. На нём и будем решать задачу линейной регрессии.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohMyWLRuKSAx",
        "outputId": "e376ba61-c33b-429b-9098-0b14d9b02013"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting memory_profiler\n",
            "  Downloading memory_profiler-0.60.0.tar.gz (38 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory_profiler) (5.4.8)\n",
            "Building wheels for collected packages: memory-profiler\n",
            "  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memory-profiler: filename=memory_profiler-0.60.0-py3-none-any.whl size=31284 sha256=081ef9953c2b6e98bcf1fee913a945b7abf4f939f2e4201d7d00098c38e7e369\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/2b/fb/326e30d638c538e69a5eb0aa47f4223d979f502bbdb403950f\n",
            "Successfully built memory-profiler\n",
            "Installing collected packages: memory-profiler\n",
            "Successfully installed memory-profiler-0.60.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install memory_profiler\n",
        "import memory_profiler\n",
        "%load_ext memory_profiler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vq7N9OSRJYI2"
      },
      "outputs": [],
      "source": [
        "file_wh = \"file_1.txt\" #https://drive.google.com/file/d/1TeLCr1wtKUKHefQWoMb3OEV-XZQeIwoE/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cjEAINWDJ2Ga"
      },
      "outputs": [],
      "source": [
        "def get_wh_points():\n",
        "  f = open(file_wh, 'r')\n",
        "\n",
        "  points_wh = []\n",
        "  for f_str in f.readlines():\n",
        "    if not f_str.isspace():\n",
        "      _, x, y = map(float, f_str.split())\n",
        "      points_wh.append(np.array([x, y]))\n",
        "  return points_wh\n",
        "\n",
        "\n",
        "def get_simple_points(size = 30):\n",
        "  points = []\n",
        "  try: \n",
        "    f = open(\"simple_file.txt\", 'r')\n",
        "    for f_str in f.readlines():\n",
        "      if not f_str.isspace():\n",
        "        _, x, y = map(float, f_str.split())\n",
        "        points.append(np.array([x, y]))\n",
        "  except:\n",
        "    for i in range(size):\n",
        "      points.append(np.array([i / 10, i / 10]))\n",
        "  return points\n",
        "\n",
        "def get_simple_points_with_up(size = 30):\n",
        "  points = []\n",
        "  try:\n",
        "    f = open(\"simple_points_with_up.txt\", 'r')\n",
        "    for f_str in f.readlines():\n",
        "      if not f_str.isspace():\n",
        "        _, x, y = map(float, f_str.split())\n",
        "        points.append(np.array([x, y]))\n",
        "  except:\n",
        "    for i in range(size - 1):\n",
        "      points.append(np.array([i / 10, i / 10]))\n",
        "    points.append(np.array([(size - 1) / 10, random.random() * 2 - 1]))\n",
        "  return points\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OEV7KPURKbcl"
      },
      "outputs": [],
      "source": [
        "def show_points(points, title=\"\"):\n",
        "  x = [p[0] for p in points]\n",
        "  y = [p[1] for p in points]\n",
        "  \n",
        "  plt.scatter(x, y, s = 10)\n",
        "  plt.title(title)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def show_points_and_line(X, Y, points, title=\"\"):\n",
        "  x = [p[0] for p in points]\n",
        "  y = [p[1] for p in points]\n",
        "  \n",
        "  plt.scatter(x, y, s = 10)\n",
        "  plt.title(title)\n",
        "  plt.plot(X, Y, color=\"g\")\n",
        "  plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaEVw1sNNGT4",
        "outputId": "1bd2b13c-a1ed-42a6-9e96-90d26cff8886"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File wasn't uploaded\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  show_points(get_wh_points(), \"Зависимость веса и роста 18-летних людей\")\n",
        "except FileNotFoundError:\n",
        "  print(\"File wasn't uploaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHcY0ybi1jsw"
      },
      "source": [
        "Будем подбирать параметры a и b линейной функции a * x + b = y, минимизируя сумму наименьших квадратов ошибок - MSE методом градиентного спуска"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-LHkCzWU1glL"
      },
      "outputs": [],
      "source": [
        "def MSE(true_points, coeffs):\n",
        "  sum = 0\n",
        "  a = coeffs[0]\n",
        "  b = coeffs[1]\n",
        "  for point in true_points:\n",
        "    sum += pow((point[1] - (point[0] * a + b)), 2) \n",
        "  return sum \n",
        "\n",
        "# её градиент\n",
        "def grad_MSE(points, coeffs, batch_size):\n",
        "  sum_a = 0\n",
        "  sum_b = 0\n",
        "  dataset_size = len(points)\n",
        "  if batch_size == dataset_size:\n",
        "    for i in range(batch_size):\n",
        "      x_i = points[i][0]\n",
        "      y_i = points[i][1]\n",
        "      sum_a += -2 * x_i * (y_i - (coeffs[0] * x_i + coeffs[1]))\n",
        "      sum_b += -2 * (y_i - (coeffs[0] * x_i + coeffs[1]))\n",
        "  else:\n",
        "    j = 0\n",
        "    while j < batch_size:\n",
        "      i = random.randint(0, dataset_size - 1)\n",
        "      x_i = points[i][0]\n",
        "      y_i = points[i][1]\n",
        "      sum_a += -2 * x_i * (y_i - (coeffs[0] * x_i + coeffs[1]))\n",
        "      sum_b += -2 * (y_i - (coeffs[0] * x_i + coeffs[1]))\n",
        "      j += 1\n",
        "    \n",
        "\n",
        "  '''print(sum_a, sum_b, \"<- sums\")'''\n",
        "  return np.array([sum_a , sum_b])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Xqx0AKb16cG"
      },
      "source": [
        "Сама реализация стохастического градиентного спуска (для начала с размером батча = n, то есть обычного градиентного спуска):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "3VvOKZePR-qv",
        "outputId": "19c1bdae-e54d-4c78-9425-8e78151f0629"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-6b01a8c8befd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mres_coeffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoeffs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcoeffs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mwh_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_wh_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mres_coeffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshocasticGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwh_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwh_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_coeffs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"<- res\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-8e50b0f64278>\u001b[0m in \u001b[0;36mget_wh_points\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_wh_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_wh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mpoints_wh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mf_str\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'file_1.txt'"
          ]
        }
      ],
      "source": [
        "import random\n",
        "const_lr = 0.0000002\n",
        "\n",
        "EPS = 1E-4\n",
        "def shocasticGD(batch_size, points, start_coeffs, lr=const_lr, epoch = 300):\n",
        "    i = 0\n",
        "    f = True\n",
        "    coeffs = start_coeffs\n",
        "    print(MSE(points, coeffs), \"<- start MSE\")\n",
        "    while i < epoch:\n",
        "      prev_coeffs = coeffs.copy()\n",
        "      coeffs -= lr * grad_MSE(points, prev_coeffs, batch_size)\n",
        "      '''print(\"res coefs for i =\", i, coeffs)'''\n",
        "      random.shuffle(points)\n",
        "      i += 1\n",
        "      if(abs(MSE(points, coeffs) - MSE(points, prev_coeffs)) < EPS and f):\n",
        "        print(\"{i} epochs were enough.\".format(i = i))\n",
        "        f = False\n",
        "    print(\"All {i} epochs were done.\".format(i = epoch))\n",
        "    print(MSE(points, coeffs), \"<- min MSE\")\n",
        "    res_coeffs = coeffs\n",
        "    return coeffs\n",
        "wh_points = get_wh_points()\n",
        "res_coeffs = shocasticGD(len(wh_points), wh_points, np.array([0.1, 0.1]))\n",
        "print(res_coeffs, \"<- res\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7w4hU4N-jdo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2pJAkxG28nX"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "Y = []\n",
        "for i in get_wh_points():\n",
        "  X.append(i[0])\n",
        "\n",
        "for i in X:\n",
        "  Y.append(i * res_coeffs[0] + res_coeffs[1])\n",
        "\n",
        "try:\n",
        "  show_points_and_line(X, Y, get_wh_points(), \"Зависимость веса и роста 18-летних людей\")\n",
        "except FileNotFoundError:\n",
        "  print(\"File wasn't uploaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pCQX6Y64FdE"
      },
      "source": [
        "Работа нашего алгоритма для точек на прямой y = x: (увеличили lr и уменьшили кол-во эпох)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCuXuSJ53AWQ"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "Y = []\n",
        "simple_points = get_simple_points()\n",
        "res_coeffs = shocasticGD(len(simple_points),simple_points, np.array([0.1, 0.1]), 0.002, 100)\n",
        "for i in simple_points:\n",
        "  X.append(i[0])\n",
        "  Y.append(res_coeffs[0] * i[0] + res_coeffs[1])\n",
        "\n",
        "\n",
        "try:\n",
        "  show_points_and_line(X, Y, simple_points, \"Прямая y = x\")\n",
        "except FileNotFoundError:\n",
        "  print(\"File wasn't uploaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oNvW4835R3E"
      },
      "source": [
        "Теперь для точек располагающихся на прямой y = x с резким выбросом одной точки:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62P1u9kU4vVd"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "Y = []\n",
        "simple_points_with_up = get_simple_points_with_up()\n",
        "res_coeffs = shocasticGD(len(simple_points_with_up), simple_points_with_up, np.array([0.1, 0.1]), 0.00002, 100)\n",
        "for i in simple_points_with_up:\n",
        "  X.append(i[0])\n",
        "  Y.append(res_coeffs[0] * i[0] + res_coeffs[1])\n",
        "\n",
        "\n",
        "try:\n",
        "  show_points_and_line(X, Y, simple_points_with_up, \"Прямая y = x с отдельной точкой\")\n",
        "except FileNotFoundError:\n",
        "  print(\"File wasn't uploaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYWGUt2Y8rcw"
      },
      "source": [
        "Теперь возьмём размер батча = 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwSgk9gt8q-U"
      },
      "outputs": [],
      "source": [
        "wh_points = get_wh_points()\n",
        "res_coeffs = shocasticGD(1, wh_points, np.array([0.1, 0.1]), epoch = 300)\n",
        "X = []\n",
        "Y = []\n",
        "for i in get_wh_points():\n",
        "  X.append(i[0])\n",
        "\n",
        "for i in X:\n",
        "  Y.append(i * res_coeffs[0] + res_coeffs[1])\n",
        "\n",
        "try:\n",
        "  show_points_and_line(X, Y, get_wh_points(), \"Зависимость веса и роста 18-летних людей\")\n",
        "except FileNotFoundError:\n",
        "  print(\"File wasn't uploaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLStBmGCAIPG"
      },
      "source": [
        "Теперь MiniBatch (n / 2):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SErYB-2Q_WpD"
      },
      "outputs": [],
      "source": [
        "wh_points = get_wh_points()\n",
        "res_coeffs = shocasticGD(len(wh_points) // 2, wh_points, np.array([0.1, 0.1]))\n",
        "X = []\n",
        "Y = []\n",
        "for i in get_wh_points():\n",
        "  X.append(i[0])\n",
        "\n",
        "for i in X:\n",
        "  Y.append(i * res_coeffs[0] + res_coeffs[1])\n",
        "\n",
        "try:\n",
        "  show_points_and_line(X, Y, get_wh_points(), \"Зависимость веса и роста 18-летних людей\")\n",
        "except FileNotFoundError:\n",
        "  print(\"File wasn't uploaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6agnnvz-fMOe"
      },
      "source": [
        "Видим, что результат линейной регрессии для размера батча = 1, вообще мало похож на правду, в то время как для miniBatch результат приближен к nGd, хотя кол-во вычислений функции в 2 раза меньше, что даёт сделать вывод о том, что miniBatch реализация является наиболее оптимальной."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBWIPbd8fDCE"
      },
      "source": [
        "# Задание 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT-x4jNbizym"
      },
      "source": [
        "Теперь нормальзуем данные:\n",
        "\n",
        "z_i = beta * z_i-1 + (1 - beta) * y_i\n",
        "\n",
        "И на них запустим наш SGD\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJczSftyfHJj"
      },
      "outputs": [],
      "source": [
        "points_wh = get_wh_points()\n",
        "#points_wh = points_wh/np.max(points_wh)\n",
        "print(points_wh)\n",
        "beta = 0.6\n",
        "for i in range(1, len(points_wh)):\n",
        "  y_prev = points_wh[i - 1][1]\n",
        "  y_i = points_wh[i][1]\n",
        "  points_wh[i][1] = beta * y_prev + (1 - beta) * y_i\n",
        "\n",
        "\n",
        "res_coeffs = shocasticGD(len(points_wh) // 2, points_wh, np.array([0.1, 0.1]))\n",
        "X = []\n",
        "Y = []\n",
        "for i in get_wh_points():\n",
        "  X.append(i[0])\n",
        "\n",
        "for i in X:\n",
        "  Y.append(i * res_coeffs[0] + res_coeffs[1])\n",
        "\n",
        "try:\n",
        "  show_points_and_line(X, Y, points_wh, \"Зависимость веса и роста 18-летних людей\")\n",
        "except FileNotFoundError:\n",
        "  print(\"File wasn't uploaded\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlkcTPrClb-7"
      },
      "source": [
        "Видим, что предварительная нормализация данных позволила сильно уменьшить результат функции ошибки MSE (с тем же кол-вом эпох и lr):\n",
        "\n",
        "с нормализацией: 1869.7431200437848 <- min MSE\n",
        "\n",
        "без: 6148.783264426831 <- min MSE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhJdYiO1mvCh"
      },
      "source": [
        "# Задание 3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPxfrxq-rNGY"
      },
      "source": [
        "(в этих модификациях метода стохастического градиентного спуска будем работать с miniBatch(n / 2), т.к. уже сделали вывод, что это самый оптимальный размер батча)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHL8bOTvmznx"
      },
      "source": [
        "SGD with momentum:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvst8tI5my-H"
      },
      "outputs": [],
      "source": [
        "EPS = 1e-3\n",
        "def stochasticGDmomentum_all(batch_size, points, start_coeffs, lr=const_lr, epoch = 100):\n",
        "  i = 0\n",
        "  coeffs = start_coeffs\n",
        "  print(MSE(points, coeffs), \"<- start MSE\")\n",
        "  td_prev=coeffs\n",
        "  beta = 0.8\n",
        "  f = True\n",
        "  mse = []\n",
        "  list_coeffs = []\n",
        "  while i < epoch:\n",
        "    list_coeffs.append(coeffs.copy())\n",
        "    mse.append(MSE(points, coeffs))\n",
        "    prev_coeffs = coeffs\n",
        "    td = beta * td_prev + lr * grad_MSE(points, prev_coeffs, batch_size)\n",
        "    td_prev = td\n",
        "    coeffs -= td\n",
        "    random.shuffle(points)\n",
        "    #print(MSE(points, coeffs))\n",
        "    i += 1\n",
        "    if(abs(MSE(points, coeffs) - MSE(points, prev_coeffs)) < EPS and f):\n",
        "      print(\"{i} epochs were enough.\".format(i = i))\n",
        "      f = False\n",
        "  if (not (i < epoch)):\n",
        "    print(\"All {i} epochs were done.\".format(i = epoch))\n",
        "  print(MSE(points, coeffs), \"<- min MSE\")\n",
        "  fig = plt.figure()\n",
        "  ax1 = fig.add_subplot(211)\n",
        "  ax1.set_xlabel('epoch')\n",
        "  ax1.plot(np.arange(epoch), mse)\n",
        "  plt.show()\n",
        "  res_coeffs = coeffs\n",
        "  return list_coeffs\n",
        "\n",
        "def stochasticGDmomentum(batch_size, points, start_coeffs, lr=const_lr, epoch = 100):\n",
        "  return stochasticGDmomentum_all(batch_size, points, start_coeffs, lr, epoch)[-1]\n",
        "\n",
        "wh_points = get_wh_points()\n",
        "res_coeffs = stochasticGDmomentum(len(wh_points) // 2, wh_points, np.array([0.1, 0.1]))\n",
        "X = []\n",
        "Y = []\n",
        "for i in get_wh_points():\n",
        "  X.append(i[0])\n",
        "\n",
        "for i in X:\n",
        "  Y.append(i * res_coeffs[0] + res_coeffs[1])\n",
        "\n",
        "try:\n",
        "  show_points_and_line(X, Y, get_wh_points(), \"Зависимость веса и роста 18-летних людей momentum\")\n",
        "except FileNotFoundError:\n",
        "  print(\"File wasn't uploaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWzbglv0nyBG"
      },
      "source": [
        "Nesterov:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cYzw87tn0pr"
      },
      "outputs": [],
      "source": [
        "def stochasticGDnesterov_all(batch_size, points, start_coeffs, lr=const_lr, epoch = 100):\n",
        "  i = 0\n",
        "  coeffs = start_coeffs\n",
        "  print(MSE(points, coeffs), \"<- start MSE\")\n",
        "  td_prev=coeffs\n",
        "  beta = 0.8\n",
        "  f = True\n",
        "  mse = []\n",
        "  list_coeffs = []\n",
        "  while i < epoch:\n",
        "    list_coeffs.append(coeffs.copy())\n",
        "    mse.append(MSE(points, coeffs))\n",
        "    prev_coeffs = coeffs\n",
        "    for j in points:\n",
        "      j -= beta * td_prev\n",
        "    td = beta * td_prev + lr * grad_MSE(points, prev_coeffs, batch_size)\n",
        "    td_prev = td\n",
        "    coeffs -= td\n",
        "    '''print(\"res coefs for i =\", i, coeffs)'''\n",
        "    random.shuffle(points)\n",
        "    i += 1\n",
        "    if(abs(MSE(points, coeffs) - MSE(points, prev_coeffs)) < EPS and f):\n",
        "      print(\"{i} epochs were enough.\".format(i = i))\n",
        "      f = False\n",
        "  fig = plt.figure()\n",
        "  ax1 = fig.add_subplot(211)\n",
        "  ax1.set_xlabel('epoch')\n",
        "  ax1.plot(np.arange(epoch), mse)\n",
        "  plt.show()\n",
        "  if (not (i < epoch)):\n",
        "    print(\"All {i} epochs were done.\".format(i = epoch))\n",
        "  print(MSE(points, coeffs), \"<- min MSE\")\n",
        "  res_coeffs = coeffs\n",
        "  return list_coeffs\n",
        "def stochasticGDnesterov(batch_size, points, start_coeffs, lr=const_lr, epoch = 100):\n",
        "  return stochasticGDnesterov_all(batch_size, points, start_coeffs, lr, epoch)[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyFQxTbrn5dU"
      },
      "outputs": [],
      "source": [
        "wh_points = get_wh_points()\n",
        "res_coeffs = stochasticGDnesterov(len(wh_points) // 2, wh_points, np.array([0.1, 0.1]))\n",
        "X = []\n",
        "Y = []\n",
        "for i in get_wh_points():\n",
        "  X.append(i[0])\n",
        "\n",
        "for i in X:\n",
        "  Y.append(i * res_coeffs[0] + res_coeffs[1])\n",
        "\n",
        "try:\n",
        "  show_points_and_line(X, Y, get_wh_points(), \"Зависимость веса и роста 18-летних людей momentum\")\n",
        "except FileNotFoundError:\n",
        "  print(\"File wasn't uploaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8mWCGDHpEGx"
      },
      "source": [
        "AdaGrad (адаптивный градиентный алгоритм):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ww4g9NdBpD8n"
      },
      "outputs": [],
      "source": [
        "def stochasticGDAdaGrad_all(batch_size, points, start_coeffs, lr=const_lr, epoch = 100):\n",
        "    i = 0\n",
        "    coeffs = start_coeffs\n",
        "    print(MSE(points, coeffs), \"<- start MSE\")\n",
        "    sum_prev=coeffs\n",
        "    mse = []\n",
        "    list_coeffs = []\n",
        "    while i < epoch:\n",
        "      list_coeffs.append(coeffs.copy())\n",
        "      mse.append(MSE(points, coeffs))\n",
        "      prev_coeffs = coeffs\n",
        "      sum = sum_prev + np.square(grad_MSE(points, prev_coeffs, batch_size))\n",
        "      sum_prev = sum\n",
        "      coeffs -= lr * np.divide(grad_MSE(points, prev_coeffs, batch_size), np.sqrt(sum))\n",
        "      random.shuffle(points)\n",
        "      i += 1\n",
        "    print(MSE(points, coeffs), \"<- min MSE\")\n",
        "    fig = plt.figure()\n",
        "    ax1 = fig.add_subplot(211)\n",
        "    ax1.set_xlabel('epoch')\n",
        "    ax1.plot(np.arange(epoch), mse)\n",
        "    plt.show()\n",
        "    res_coeffs = coeffs\n",
        "    return list_coeffs\n",
        "def stochasticGDAdaGrad(batch_size, points, start_coeffs, lr=const_lr, epoch = 100):\n",
        "  return stochasticGDAdaGrad_all(batch_size, points, start_coeffs, lr, epoch)[-1]\n",
        "wh_points = get_wh_points()\n",
        "res_coeffs = stochasticGDAdaGrad(len(wh_points) // 2, wh_points, np.array([0.1, 0.1]))\n",
        "X = []\n",
        "Y = []\n",
        "print(res_coeffs, \"<- res\")\n",
        "for i in get_wh_points():\n",
        "  X.append(i[0])\n",
        "\n",
        "for i in X:\n",
        "  Y.append(i * res_coeffs[0] + res_coeffs[1])\n",
        "\n",
        "try:\n",
        "  show_points_and_line(X, Y, wh_points, \"Зависимость веса и роста 18-летних людей adaGrad со старым lr\")\n",
        "except FileNotFoundError:\n",
        "  print(\"File wasn't uploaded\")\n",
        "\n",
        "wh_points = get_wh_points()\n",
        "res_coeffs = stochasticGDAdaGrad(len(wh_points) // 2, wh_points, np.array([0.1, 0.1]), lr=1.5)\n",
        "X = []\n",
        "Y = []\n",
        "print(res_coeffs, \"<- res\")\n",
        "for i in get_wh_points():\n",
        "  X.append(i[0])\n",
        "\n",
        "for i in X:\n",
        "  Y.append(i * res_coeffs[0] + res_coeffs[1])\n",
        "\n",
        "try:\n",
        "  show_points_and_line(X, Y, wh_points, \"Зависимость веса и роста 18-летних людей adaGrad с увеличенным lr\")\n",
        "except FileNotFoundError:\n",
        "  print(\"File wasn't uploaded\")\n",
        "\n",
        "linear_points = get_simple_points()\n",
        "res_coeffs = stochasticGDAdaGrad(len(linear_points) // 2, linear_points, np.array([0.1, 0.1]), lr=1.5)\n",
        "X = []\n",
        "Y = []\n",
        "print(res_coeffs, \"<- res\")\n",
        "for i in linear_points:\n",
        "  X.append(i[0])\n",
        "\n",
        "for i in X:\n",
        "  Y.append(i * res_coeffs[0] + res_coeffs[1])\n",
        "\n",
        "try:\n",
        "  show_points_and_line(X, Y, linear_points, \"Зависимость веса и роста 18-летних людей adaGrad с увеличенным lr\")\n",
        "except FileNotFoundError:\n",
        "  print(\"File wasn't uploaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qPq8a6-1Zb7"
      },
      "source": [
        "Здесь с lr из предыдущего шага алгоритм не работает так, как нужно. С увеличением learning rate константы всё работает."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWBnu7U7xHgw"
      },
      "source": [
        "RMSProp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZQLwTDtxJco"
      },
      "outputs": [],
      "source": [
        "def stochasticGDRMS_all(batch_size, points, start_coeffs, lr=0.1, epoch = 100):\n",
        "  i = 0\n",
        "  gamma = 0.9\n",
        "  coeffs = start_coeffs\n",
        "  print(MSE(points, coeffs), \"<- start MSE\")\n",
        "  V_prev=coeffs\n",
        "  mse = []\n",
        "  list_coeff = []\n",
        "  while i < epoch:\n",
        "    list_coeff.append(coeffs.copy())\n",
        "    mse.append(MSE(points, coeffs))\n",
        "    prev_coeffs = coeffs\n",
        "    V = gamma * V_prev + (1 - gamma) * np.square(grad_MSE(points, prev_coeffs, batch_size))\n",
        "    sum_prev = sum\n",
        "    coeffs -= lr * np.divide(grad_MSE(points, prev_coeffs, batch_size), np.sqrt(V))\n",
        "    random.shuffle(points)\n",
        "    i += 1\n",
        "  print(MSE(points, coeffs), \"<- min MSE\")\n",
        "  fig = plt.figure()\n",
        "  ax1 = fig.add_subplot(211)\n",
        "  ax1.set_xlabel('epoch')\n",
        "  ax1.plot(np.arange(epoch), mse)\n",
        "  plt.show()\n",
        "  res_coeffs = coeffs\n",
        "  return list_coeff\n",
        "def stochasticGDRMS(batch_size, points, start_coeffs, lr=0.1, epoch = 100):\n",
        "  return stochasticGDRMS_all(batch_size, points, start_coeffs, lr, epoch)[-1]\n",
        "wh_points = get_wh_points()\n",
        "res_coeffs = stochasticGDRMS(len(wh_points) // 2, wh_points, np.array([0.1, 0.1]))\n",
        "X = []\n",
        "Y = []\n",
        "print(res_coeffs, \"<- res\")\n",
        "for i in get_wh_points():\n",
        "  X.append(i[0])\n",
        "\n",
        "for i in X:\n",
        "  Y.append(i * res_coeffs[0] + res_coeffs[1])\n",
        "\n",
        "try:\n",
        "  show_points_and_line(X, Y, wh_points, \"Зависимость веса и роста 18-летних людей RMSProp\")\n",
        "except FileNotFoundError:\n",
        "  print(\"File wasn't uploaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2QK551r9TXQ"
      },
      "source": [
        "Аналогично предыдущему методу: с увеличением константы всё работает, но с констанотой из первых 2-ух методов всё плохо :("
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPpe99O79hTJ"
      },
      "source": [
        "Adam (adaptive moment estimation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjRWM3QK9k0B"
      },
      "outputs": [],
      "source": [
        "def stochasticAdam_all(batch_size, points, start_coeffs, lr=0.1, epoch = 100, f = MSE, grad = grad_MSE):\n",
        "    i = 0\n",
        "    gamma = 0.99\n",
        "    coeffs = start_coeffs\n",
        "    td_prev = start_coeffs\n",
        "    beta = 0.9\n",
        "    print(f(points, coeffs), \"<- start MSE\")\n",
        "    V_prev=coeffs\n",
        "    mse = []\n",
        "    list_coeff = []\n",
        "    while i < epoch:\n",
        "      list_coeff.append(coeffs.copy())\n",
        "      mse.append(f(points, coeffs))\n",
        "      prev_coeffs = coeffs\n",
        "      td = beta * td_prev + (1 - beta) * grad(points, prev_coeffs, batch_size)\n",
        "      V = gamma * V_prev + (1 - gamma) * np.square(grad(points, prev_coeffs, batch_size))\n",
        "      coeffs -= lr * np.divide(td, np.sqrt(V) + 1E-8)\n",
        "      random.shuffle(points)\n",
        "      i += 1\n",
        "    print(f(points, coeffs), \"<- min MSE\")\n",
        "    fig = plt.figure()\n",
        "    ax1 = fig.add_subplot(211)\n",
        "    ax1.set_xlabel('epoch')\n",
        "    ax1.plot(np.arange(epoch), mse)\n",
        "    plt.show()\n",
        "    res_coeffs = coeffs\n",
        "    return list_coeff\n",
        "def stochasticAdam(batch_size, points, start_coeffs, lr=0.1, epoch = 100, f = MSE, grad = grad_MSE):\n",
        "  return stochasticAdam_all(batch_size, points, start_coeffs, lr, epoch, f, grad)[-1]\n",
        "wh_points = get_wh_points()\n",
        "res_coeffs = stochasticAdam(len(wh_points) // 2, wh_points, np.array([0.1, 0.1]))\n",
        "X = []\n",
        "Y = []\n",
        "print(res_coeffs, \"<- res\")\n",
        "for i in get_wh_points():\n",
        "  X.append(i[0])\n",
        "\n",
        "for i in X:\n",
        "  Y.append(i * res_coeffs[0] + res_coeffs[1])\n",
        "\n",
        "try:\n",
        "  show_points_and_line(X, Y, wh_points, \"Зависимость веса и роста 18-летних людей Adam\")\n",
        "except FileNotFoundError:\n",
        "  print(\"File wasn't uploaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icJpjzBp_4Dy"
      },
      "source": [
        "Аналогично предыдущему методу: с увеличением константы всё работает, но с констанотой из первых 2-ух методов всё плохо :("
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWFE2B4cIK6C"
      },
      "source": [
        "## ***4. Сравнить методы.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cu0REkBK1uU"
      },
      "outputs": [],
      "source": [
        "print('SGD with momentum')\n",
        "%time %memit stochasticGDmomentum(len(wh_points) // 2, wh_points, np.array([0.1, 0.1]))\n",
        "print('Nesterov')\n",
        "%time %memit stochasticGDnesterov(len(wh_points) // 2, wh_points, np.array([0.1, 0.1]))\n",
        "print('AdaGrad')\n",
        "%time %memit stochasticGDAdaGrad(len(wh_points) // 2, wh_points, np.array([0.1, 0.1]))\n",
        "print('RMSProp')\n",
        "%time %memit stochasticGDRMS(len(wh_points) // 2, wh_points, np.array([0.1, 0.1]))\n",
        "print('Adam')\n",
        "%time %memit stochasticAdam(len(wh_points) // 2, wh_points, np.array([0.1, 0.1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efp7YpEjISE5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "'''df = pd.DataFrame([[38.0, 2.0, 18.0, 22.0, 21, np.nan],[19, 439, 6, 452, 226,232]],\n",
        "                  index=pd.Index(['Tumour (Positive)', 'Non-Tumour (Negative)'], name='Actual Label:'),\n",
        "                  columns=pd.MultiIndex.from_product([['Decision Tree', 'Regression', 'Random'],['Tumour', 'Non-Tumour']], names=['Model:', 'Predicted:']))\n",
        "[[\"20 итераций\", \"надёжный\", 189.13, \"\", 10],\n",
        "                    [\"20 итераций\", \"надёжный\", 189.13, 10, 10],\n",
        "                    [\"5 итераций\", \"надёжный\", 189.13, 10, 10],\n",
        "                    [\"в среднем 25\", \"ненадёжный\", 189.13, 10, 10],\n",
        "                    [\"в среднем 25\", \"ненадёжный\", 189.13, 10, 10]],\n",
        "'''\n",
        "df = pd.DataFrame([[\"20 итераций\", \"надёжный\", 189.13, 42.7],\n",
        "                    [\"20 итераций\", \"надёжный\", 189.13, 54],\n",
        "                    [\"5 итераций\", \"надёжный\", 189.13, 48.6],\n",
        "                    [\"в среднем 25\", \"ненадёжный\", 189.13, 48.4],\n",
        "                    [\"в среднем 25\", \"ненадёжный\", 189.13, 49.6]],\n",
        "                  index=pd.Index(['Momentum', 'Nesterov', 'AdaGrad', 'RMSProp', 'Adam'], name='Models:'),\n",
        "                  columns=['Скорость сходимости', 'Надёжность', 'Объем оперативной памяти','Время выполнения'])\n",
        "df.style"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R82gvJSDcd3E"
      },
      "source": [
        "## ***5. График***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mucqDx7Zcjgn"
      },
      "outputs": [],
      "source": [
        "space = np.linspace(-10, 10, 100)\n",
        "x_def, y_def = np.meshgrid(space, space)\n",
        "v_def = np.array([x_def, y_def])\n",
        "padding = 5\n",
        "def momentum():\n",
        "  return stochasticGDmomentum_all(len(wh_points) // 2, wh_points, np.array([0.1, 0.1]))\n",
        "def nesterov():\n",
        "  return stochasticGDnesterov_all(len(wh_points) // 2, wh_points, np.array([0.1, 0.1]))\n",
        "def adagrad():\n",
        "  return stochasticGDAdaGrad_all(len(wh_points) // 2, wh_points, np.array([0.1, 0.1]), lr=1.5)\n",
        "def rmsprop():\n",
        "  return stochasticGDRMS_all(len(wh_points) // 2, wh_points, np.array([0.1, 0.1]))\n",
        "def adam():\n",
        "  return stochasticAdam_all(len(wh_points) // 2, wh_points, np.array([0.1, 0.1]))\n",
        "def draw_graphic(f, array):\n",
        "  plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
        "  ogr = [float('inf'), float('-inf'), float('inf'), float('-inf'), float('inf'), float('-inf')]\n",
        "  near_levels = []\n",
        "  for coeffs in array:\n",
        "    x_list = coeffs.transpose()[0]\n",
        "    y_list = coeffs.transpose()[1]\n",
        "    res = f(wh_points, v_def)\n",
        "    this_ogr = [np.min(x_list), np.max(x_list), np.min(y_list), np.max(y_list), np.min(res), np.max(res)]\n",
        "    for i in range(6):\n",
        "      if i%2 == 0:\n",
        "        if (this_ogr[i] < ogr[i]):\n",
        "          ogr[i] = this_ogr[i]\n",
        "      else:\n",
        "        if (this_ogr[i] > ogr[i]):\n",
        "          ogr[i] = this_ogr[i]\n",
        "\n",
        "    '''horisontal_size = max_x - min_x\n",
        "    vertical_size = max_y - min_y\n",
        "    space_x = np.linspace(min_x-horisontal_size/4, max_x+horisontal_size/4, int(100))\n",
        "    space_y = np.linspace(min_y-vertical_size/4, max_y+vertical_size/4, int(100))\n",
        "    space = np.linspace(-10, 10, 100)\n",
        "    x, y = np.meshgrid(space_x, space_y)\n",
        "    v = np.array([x, y])\n",
        "    res = f(wh_points, v)\n",
        "    minf = np.min(res)\n",
        "    maxf = np.max(res)'''\n",
        "    near_levels += [f(wh_points, p) for p in coeffs[::3]]\n",
        "    plt.plot(coeffs[:, 0], coeffs[:, 1], 'o-')\n",
        "    '''ax = plt.figure().add_subplot()\n",
        "    ax.plot(coeffs[:, 0], coeffs[:, 1], 'o-')\n",
        "    print([f(wh_points, p) for p in coeffs] )\n",
        "    ax.contour(x, y, f(wh_points, v), levels = np.unique(sorted(list(np.linspace(minf, maxf, 100)))))''' #[f(wh_points, p) for p in coeffs] +\n",
        "  final_ogr = [max(-10, ogr[0]) - ogr[0] / padding, min(10, ogr[1]) - ogr[1] / padding, max(-10, ogr[2]) - ogr[2] / padding, min(10, ogr[3]) + ogr[3] / padding]\n",
        "  space_x = np.linspace(final_ogr[0], final_ogr[1], 100)\n",
        "  space_y = np.linspace(final_ogr[2], final_ogr[3], 100)\n",
        "  x, y = np.meshgrid(space_x, space_y)\n",
        "  v = np.array([x, y])\n",
        "  plt.xlim(final_ogr[0], final_ogr[1])\n",
        "  plt.ylim(final_ogr[2], final_ogr[3])\n",
        "  plt.contour(x, y, f(wh_points, v), levels = np.unique(sorted(near_levels + list(np.linspace(ogr[4], ogr[5], 1000)))))\n",
        "  plt.show()\n",
        "\n",
        "draw_graphic(MSE, np.array([momentum(), nesterov(), adagrad(), rmsprop(), adam()]))\n",
        "'''\n",
        "adam - фиолетовый, \n",
        "rmsprop - красный, \n",
        "adagrad - зелёный, \n",
        "nesterov - оранжевый, \n",
        "momentum - синий'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}